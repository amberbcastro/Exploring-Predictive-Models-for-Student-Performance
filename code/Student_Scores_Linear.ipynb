{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95966a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "exam1 = pd.read_csv(\"Exam_1_scores.csv\")\n",
    "exam2 = pd.read_csv(\"Exam_2_scores.csv\")\n",
    "exam3 = pd.read_csv(\"Exam_3_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fbcf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only graded entries\n",
    "exam1 = exam1[exam1['Status'] == 'Graded']\n",
    "exam2 = exam2[exam2['Status'] == 'Graded']\n",
    "exam3 = exam3[exam3['Status'] == 'Graded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing score so they can be compared\n",
    "exam1['Exam1'] = (exam1['Total Score'] / exam1['Max Points'])*100\n",
    "exam2['Exam2'] = (exam2['Total Score'] / exam2['Max Points'])*100\n",
    "exam3['Exam3'] = (exam3['Total Score'] / exam3['Max Points'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672a988",
   "metadata": {},
   "outputs": [],
   "source": [
    "exam1 = exam1[['SID', 'Exam1']]\n",
    "exam2 = exam2[['SID', 'Exam2']]\n",
    "exam3 = exam3[['SID', 'Exam3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c981e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = exam1.merge(exam2, on='SID').merge(exam3, on='SID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd176a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of Exam 1, 2, and 3 Scores\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(merged['Exam1'], merged['Exam2'], merged['Exam3'], c='blue', alpha=0.6)\n",
    "ax.set_xlabel('Exam 1 Score')\n",
    "ax.set_ylabel('Exam 2 Score')\n",
    "ax.set_zlabel('Exam 3 Score')\n",
    "ax.set_title('3D Visualization of Exam Scores')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d2827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = merged[['Exam1', 'Exam2']]\n",
    "y = merged['Exam3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0d2028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6451862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073c634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"MAE:  {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"RÂ²:   {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561952c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual Exam 3 Scores\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Perfect Prediction')\n",
    "plt.xlabel(\"Actual Exam 3 Score\")\n",
    "plt.ylabel(\"Predicted Exam 3 Score\")\n",
    "plt.title(\"Linear Regression: Predicted vs Actual Exam 3\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ab6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print coefficients\n",
    "print(\"\\nModel Coefficients:\")\n",
    "print(f\"Intercept: {model.intercept_:.2f}\")\n",
    "print(f\"Exam1 Weight: {model.coef_[0]:.2f}\")\n",
    "print(f\"Exam2 Weight: {model.coef_[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbfc9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
